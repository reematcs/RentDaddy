name: AWS Deploy

on:
  push:
    branches:
      - main
  workflow_dispatch:

env:
  AWS_REGION: us-east-2
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

jobs:
  terraform:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    environment: production
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"
      
      - name: Terraform Init
        working-directory: ./deployment/simplified_terraform
        run: |
          # Use the GitHub Actions specific Terraform file
          mv main.tf main-local.tf.backup || true
          mv main-github-actions.tf main.tf
          terraform init
      
      - name: Import Existing Resources
        working-directory: ./deployment/simplified_terraform
        run: |
          echo "Checking for existing resources that need to be imported..."
          
          # Function to check and import resource
          check_and_import() {
            local resource_type=$1
            local resource_name=$2
            local resource_id=$3
            local check_command=$4
            
            # Check if resource exists in state
            if ! terraform state show "${resource_type}.${resource_name}" &>/dev/null; then
              # Check if resource exists in AWS
              if eval "$check_command" &>/dev/null; then
                echo "Importing ${resource_type}.${resource_name}..."
                terraform import -var="aws_account_id=${{ env.AWS_ACCOUNT_ID }}" "${resource_type}.${resource_name}" "$resource_id" || true
              fi
            fi
          }
          
          # Import ALB if it exists
          ALB_ARN=$(aws elbv2 describe-load-balancers --names rentdaddy-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
          if [ "$ALB_ARN" != "" ] && [ "$ALB_ARN" != "None" ]; then
            check_and_import "aws_lb" "main" "$ALB_ARN" "aws elbv2 describe-load-balancers --names rentdaddy-alb"
          fi
          
          # Import Target Groups
          BACKEND_TG_ARN=$(aws elbv2 describe-target-groups --names backend-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ "$BACKEND_TG_ARN" != "" ] && [ "$BACKEND_TG_ARN" != "None" ]; then
            check_and_import "aws_lb_target_group" "backend" "$BACKEND_TG_ARN" "aws elbv2 describe-target-groups --names backend-tg"
          fi
          
          DOCUMENSO_TG_ARN=$(aws elbv2 describe-target-groups --names documenso-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ "$DOCUMENSO_TG_ARN" != "" ] && [ "$DOCUMENSO_TG_ARN" != "None" ]; then
            check_and_import "aws_lb_target_group" "documenso" "$DOCUMENSO_TG_ARN" "aws elbv2 describe-target-groups --names documenso-tg"
          fi
          
          # Import ECS Cluster
          check_and_import "aws_ecs_cluster" "main" "rentdaddy-cluster" "aws ecs describe-clusters --clusters rentdaddy-cluster --query 'clusters[0]'"
          
          # Import IAM Roles
          check_and_import "aws_iam_role" "ecs_task_execution" "rentdaddy-ecs-task-execution-role" "aws iam get-role --role-name rentdaddy-ecs-task-execution-role"
          check_and_import "aws_iam_role" "ecs_task" "rentdaddy-ecs-task-role" "aws iam get-role --role-name rentdaddy-ecs-task-role"
          check_and_import "aws_iam_role" "ecs_instance" "rentdaddy-ecs-instance-role" "aws iam get-role --role-name rentdaddy-ecs-instance-role"
          
          # Import CloudWatch Log Groups
          check_and_import "aws_cloudwatch_log_group" "backend_logs" "/ecs/rentdaddy-backend" "aws logs describe-log-groups --log-group-name-prefix /ecs/rentdaddy-backend --query \"logGroups[?logGroupName=='/ecs/rentdaddy-backend']\""
          check_and_import "aws_cloudwatch_log_group" "frontend_logs" "/ecs/rentdaddy-frontend" "aws logs describe-log-groups --log-group-name-prefix /ecs/rentdaddy-frontend --query \"logGroups[?logGroupName=='/ecs/rentdaddy-frontend']\""
          check_and_import "aws_cloudwatch_log_group" "documenso_logs" "/ecs/rentdaddy-documenso" "aws logs describe-log-groups --log-group-name-prefix /ecs/rentdaddy-documenso --query \"logGroups[?logGroupName=='/ecs/rentdaddy-documenso']\""
          
          echo "Import check complete"
      
      - name: Terraform Plan
        working-directory: ./deployment/simplified_terraform
        run: |
          terraform plan \
            -var="aws_account_id=${{ env.AWS_ACCOUNT_ID }}" \
            -out=tfplan
      
      - name: Terraform Apply
        working-directory: ./deployment/simplified_terraform
        if: github.event_name == 'push' || github.event.inputs.deploy == 'true'
        run: |
          terraform apply -auto-approve tfplan

  build-and-deploy:
    name: Build and Deploy Applications
    runs-on: ubuntu-latest
    needs: terraform
    environment: production
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Build and Push Backend
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Create .env file for build
          cat > backend/.env.production.local << EOF
          POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}
          CLERK_SECRET_KEY=${{ secrets.CLERK_SECRET_KEY }}
          CLERK_WEBHOOK=${{ secrets.CLERK_WEBHOOK }}
          SMTP_USER=${{ secrets.SMTP_USER }}
          SMTP_PASSWORD=${{ secrets.SMTP_PASSWORD }}
          DOCUMENSO_API_KEY=${{ secrets.DOCUMENSO_API_KEY }}
          DOCUMENSO_WEBHOOK_SECRET=${{ secrets.DOCUMENSO_WEBHOOK_SECRET }}
          PG_URL=${{ secrets.PG_URL }}
          # Add other secrets as needed
          EOF
          
          cd backend
          docker build -t $ECR_REGISTRY/rentdaddy/backend:$IMAGE_TAG -f Dockerfile.prod .
          docker tag $ECR_REGISTRY/rentdaddy/backend:$IMAGE_TAG $ECR_REGISTRY/rentdaddy/backend:latest
          docker push $ECR_REGISTRY/rentdaddy/backend:$IMAGE_TAG
          docker push $ECR_REGISTRY/rentdaddy/backend:latest
      
      - name: Build and Push Frontend
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Create .env file for build
          cat > frontend/app/.env.production.local << EOF
          VITE_CLERK_PUBLISHABLE_KEY=${{ vars.VITE_CLERK_PUBLISHABLE_KEY }}
          VITE_BACKEND_URL=${{ vars.VITE_BACKEND_URL }}
          VITE_PORT=${{ vars.VITE_PORT }}
          VITE_DOMAIN_URL=${{ vars.VITE_DOMAIN_URL }}
          VITE_DOCUMENSO_PUBLIC_URL=${{ vars.VITE_DOCUMENSO_PUBLIC_URL }}
          VITE_SERVER_URL=${{ vars.VITE_SERVER_URL }}
          VITE_ENV=production
          EOF
          
          cd frontend/app
          docker build -t $ECR_REGISTRY/rentdaddy/frontend:$IMAGE_TAG -f Dockerfile.prod .
          docker tag $ECR_REGISTRY/rentdaddy/frontend:$IMAGE_TAG $ECR_REGISTRY/rentdaddy/frontend:prod
          docker push $ECR_REGISTRY/rentdaddy/frontend:$IMAGE_TAG
          docker push $ECR_REGISTRY/rentdaddy/frontend:prod
      
      - name: Force ECS Deployment
        run: |
          aws ecs update-service \
            --cluster rentdaddy-cluster \
            --service rentdaddy-app-service \
            --force-new-deployment
          
          aws ecs update-service \
            --cluster rentdaddy-cluster \
            --service rentdaddy-documenso-service \
            --force-new-deployment
      
      - name: Wait for Deployment
        timeout-minutes: 10
        run: |
          echo "Waiting for services to stabilize..."
          aws ecs wait services-stable \
            --cluster rentdaddy-cluster \
            --services rentdaddy-app-service rentdaddy-documenso-service
      
      - name: Verify Deployment
        run: |
          echo "Checking service health..."
          
          # Check frontend
          response=$(curl -s -o /dev/null -w "%{http_code}" https://app.curiousdev.net -m 10)
          if [ "$response" != "200" ]; then
            echo "Frontend health check failed with status $response"
            exit 1
          fi
          echo "✅ Frontend is healthy"
          
          # Check API
          response=$(curl -s -o /dev/null -w "%{http_code}" https://api.curiousdev.net/healthz -m 10)
          if [ "$response" != "200" ]; then
            echo "API health check failed with status $response"
            exit 1
          fi
          echo "✅ API is healthy"
          
          # Check Documenso (expecting 307 redirect)
          response=$(curl -s -o /dev/null -w "%{http_code}" https://docs.curiousdev.net -m 10)
          if [[ "$response" != "3"* ]]; then
            echo "Documenso health check failed with status $response"
            exit 1
          fi
          echo "✅ Documenso is healthy"